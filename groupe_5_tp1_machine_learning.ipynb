{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29d1cb44",
   "metadata": {},
   "source": [
    "# Groupe5 - Apprentissage Artificiel\n",
    "\n",
    "### FORDJOU KAMGANG Landry                            19M2366\n",
    "### PANDJI TCHOUAKOUE Frank Manuel               19M2110\n",
    "### AZESSIE NOGHIEWO Léonce                             19M2617\n",
    "### EYENGA MINKONDA Laurentine Serena           19M2455"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea4a5bb",
   "metadata": {},
   "source": [
    "## TP1: Spam Email Classifier with KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a5f105",
   "metadata": {},
   "source": [
    "### Importation des bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c797c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6def7329",
   "metadata": {},
   "source": [
    "### 1- Chargement des donées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61a95f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    print(\"Loading data...\")\n",
    "    #ham_files_location = os.listdir(\"enron2/ham\")\n",
    "    #spam_files_location = os.listdir(\"enron2/spam\")\n",
    "    spam_files_location = os.listdir(\"enron2/spam\")\n",
    "    ham_files_location = os.listdir(\"enron2/ham\")\n",
    "    data = []\n",
    "    \n",
    "    # Load ham email\n",
    "    for file_path in ham_files_location:\n",
    "        f = open(\"enron2/ham/\" + file_path, \"r\")\n",
    "        text = str(f.read())\n",
    "        data.append([text, \"ham\"])\n",
    "    \n",
    "    # Load spam email\n",
    "    for file_path in spam_files_location:\n",
    "        f = open(\"enron2/spam/\" + file_path, \"r\")\n",
    "        text = str(f.read())\n",
    "        data.append([text, \"spam\"])\n",
    "        \n",
    "    data = np.array(data)\n",
    "    print(\"flag 1: loaded data\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61e685ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "flag 1: loaded data\n",
      "5857\n"
     ]
    }
   ],
   "source": [
    "mails = load_data()\n",
    "print(len(mails))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cffa05e",
   "metadata": {},
   "source": [
    "### 2- Pré-traitement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4c0ee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('brown')\n",
    "#nltk.download()\n",
    "#brown.words()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4d23b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing data: noise removal\n",
    "def preprocess_data(data):\n",
    "    print(\"Preprocessing data...\") \n",
    "    punc = string.punctuation           # Punctuation list\n",
    "    sw = stopwords.words('english')     # Stopwords list\n",
    "    for record in data:\n",
    "        # Remove common punctuation and symbols\n",
    "        for item in punc:\n",
    "            record[0] = record[0].replace(item, \"\")\n",
    "        # Lowercase all letters and remove stopwords \n",
    "            splittedWords = record[0].split()\n",
    "            newText = \"\"\n",
    "            for word in splittedWords:\n",
    "                if word not in sw:\n",
    "                    word = word.lower()\n",
    "                    newText = newText + \" \" + word\n",
    "            record[0] = newText\n",
    "    print(\"flag 2: preprocessed data\")        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "68aa9436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-ec899f20cd15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpreprocess_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmails\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-74-a8fc40a2e9d1>\u001b[0m in \u001b[0;36mpreprocess_data\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msplittedWords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msw\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                     \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                     \u001b[0mnewText\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnewText\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0mrecord\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnewText\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "preprocess_data(mails)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbf3d58",
   "metadata": {},
   "source": [
    "### Découpage du jeu de données en jeu d' apprentissage et jeu de test\n",
    "Le découpage est fait dans les proportions 73%(apprentissage) - 27%(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e19411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting original dataset into training dataset and test dataset\n",
    "def split_data(data):\n",
    "    print(\"Splitting data...\")\n",
    "    features = data[:, 0]   # array containing all email text bodies\n",
    "    labels = data[:, 1]     # array containing corresponding labels\n",
    "    print(labels)\n",
    "    training_data, test_data, training_labels, test_labels =\\\n",
    "        train_test_split(features, labels, test_size = 0.27, random_state = 42)\n",
    "    \n",
    "    print(\"flag 3: splitted data\")\n",
    "    return training_data, test_data, training_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9e4182b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data...\n",
      "['ham' 'ham' 'ham' ... 'spam' 'spam' 'spam']\n",
      "flag 3: splitted data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([' subject new offrr want know ho liberalize w save 60 northward dlcatlons http www centr compassion alpan com successfull proven way rocking monumentalize ave money splash st prlces high qua homophone iity w grundyism orldwide shlpplng total confidenti despoil aiity 200 appeasement popular medlcatlons nice da upheave',\n",
       "        ' subject enron default swaps hi vince got notes indeed useful one deutsche bank especially helpful suppose know stuff teach sorry delayed billing trouble getting bill excellent asistant taichi hoshino returned goldman tokyo able get anything else done lately try get something soon several energy people several companies credit risk exec ed course last month seems credit risk power risk go together days warm regards darrell fri 30 mar 2001 vince j kaminski enron com wrote darrell sending 2 technical notes enron default swaps hope useful shall read articles weekend curious find explanations satisfactory slow preparing number technical documents model reviews still hope able find time review credit models london credit trading var option pricing related models also please check invoices still think owe money vince see attached file cds vs pdf see attached file cdsstrat pdf darrell duffie 03 28 2001 08 07 38 vince j kaminski cc subject enron default swaps vince according bank america publication enron default swap spreads consistently trading 80 basis points wider asset swaps idea going thanks guidance darrell darrell duffie mail gsb stanford ca 94305 5015 usa phone 650 723 1976 fax 650 725 7979 email duffie stanford edu web http www stanford edu duffie darrell duffie mail gsb stanford ca 94305 5015 usa phone 650 723 1976 fax 650 725 7979 email duffie stanford edu web http www stanford edu duffie',\n",
       "        ' subject application expires july 27 application grant remember type grant never need repay time limited must place order midnight saturday july 27 2002 order secure place programs many people qualify program limiting initial applicants serious sincere honest individuals ensure program money used beneficial constructive uses remember risk part also grant usually minimum 10 000 great opportunity see eligible larger grant qualify free grant program lose nothing even apply lose everything remember everyone gets opportunity get one first people apply chances much higher apply deadline almost',\n",
       "        ...,\n",
       "        ' subject software taking bite budget try oem want learn build website literature news stays news reward thing well done done',\n",
       "        ' subject would like 250 gas card let current high price gas get simply enter zipcode see promotion available area qkppfiui',\n",
       "        ' subject interviews hi vince thanks taking time meet last week enjoy meeting co workers research group realize background may best fit type work done division job hunting next several weeks would really appreciate could let let know something opens enron thanks best regards ruwan ruwan jayasuriya economics department ruwan rice edu rice university http www ruf rice edu ruwan houston tx 77005'],\n",
       "       dtype='<U43816'),\n",
       " array([' subject save money buy getting thing tried cialls yet cannot even imagine like real man bed thing great errrectlon provided exactiy want cialis lot advantages viagra effect lasts 36 hours ready start within 10 minutes mix alcohol ship country get riqht',\n",
       "        ' subject recommendation outstanding baylor mba student summer internship 10 17 3 31 2001 0600 vince j kaminski enron com wrote jim shall contact althea make sure rusty meets research group members vince vince quite grateful thanks arranging way chance might able attract campus spring school lets let definitely plan scheduling time coming fall 2001 semester visit campus present lecture risk management students sincerely jim garven james r garven ph professor finance insurance department finance insurance real estate hankamer school business hsb 336 baylor university box 98004 waco tx 76798 voice 254 710 6207 fax 320 213 5580 e mail james garven baylor edu home page http garven baylor edu vita http garven baylor edu dossier html research paper archive http garven baylor edu research html',\n",
       "        ' subject preface book vince hope well spoke ago write preface book kindly offered would provide still possible realise extremely busy chris les went ahead wrote something want review change write preface would appreciated let know thoughts thanks julie getting close preface one main objectives writing energy derivatives pricing risk management bring together many various approaches pricing risk management energy derivatives possible discuss depth models show relate way hope help reader analyse different models price wide range energy derivatives build risk management system uses consistent modelling framework believe practitioners last point important continue stress articles presentations dangers flawed risk management giving arbitrage opportunities competitors using ad hoc inconsistent models different instruments markets see also others propose consistent models however wish concentrate one particular model models exclusion others believe choice rest user although probably clear discussions model prefer therefore try give clear account possible advantage disadvantages models reader make informed choice models best suit needs order meet objectives book divided 11 chapters chapter 1 give overview fundamental principals needed model price energy derivatives underpin remainder book addition introducing techniques underlie black scholes modelling framework outline numerical techniques trinomial trees monte carlo simulation derivative pricing used throughout book chapter 2 discuss analysis spot energy prices well analysing empirical price movements propose number processes used model prices look well know process geometric brownian motion well mean reversion stochastic volatility jump processes discussing showing simulated parameters estimated chapter 3 written vince kaminski grant masson ronnie chahal enron corp discusses volatility estimation energy commodity markets chapter builds previous one examines detail methods merits pitfalls volatility estimation process assuming different pricing models introduced chapter 2 examples crude gas electricity markets used illustrate technical interpretative aspects calculating volatility chapter 4 examines forward curves energy markets although curves well understood straight forward financial markets difficulty storage many energy markets leads less well defined curves chapter describe forward price bounds energy prices building forward curves market instruments outline three main approaches applied building forward curves energy markets arbitrage approach econometric approach deriving analytical values modelling underlying stochastic factors chapter 5 presents overview structures found energy derivative markets discusses uses examples products analysed chapter include variety swaps caps floors collars well energy swaptions compound options asian options barrier options lookback options ladder options chapter 6 investigates single multi factor models energy spot price pricing standard energy derivatives closed form solutions forward prices forward volatilities european option prices spot forwards derived presented models chapter including three factor stochastic convenience yield interest rate model chapter 7 shows prices path dependent american style options evaluated models chapter 6 simulation schemes developed evaluation european style options applied variety path dependent options order price options incorporate early exercise opportunities trinomial tree scheme developed tree built consistent observed forward curve used price exotic well standard european american style options chapter 8 describes methodology valuing energy options based modelling whole market observed forward curve approach results multi factor model able realistically capture evolution wide range energy forward curves user defined volatility structures extremely general form closed form solutions developed pricing standard european options efficient monte carlo schemes presented pricing exotic options chapter closes discussion valuation american style options chapter 9 focuses risk management energy derivative positions chapter discuss management price risk institutions trade options derivatives faced problem managing risk time begin delta hedging portfolio containing derivatives look extensions gamma hedging \\x01 illustrating techniques using spot forward curve models general model presented chapter 8 ideally suited multi factor hedging portfolio energy derivatives also discussed chapter 10 examines key risk management concept value risk var applied portfolios containing energy derivative products discussing concept measure look key inputs volatilities covariances correlations etc estimated compare fours major methodologies computing var delta delta gamma historical simulation monte carlo simulation applying portfolio energy options chapter also look testing var estimates various underlying energy market variables finally chapter 11 review modelling approaches credit risk look detail two quite different approaches creditmetrics j p morgan 1997 creditrisk credit suisse financial products 1997 detailed information publicly available together provide extensive set tools measure credit risk present numerical examples applying techniques energy derivatives begin stress models methods present book tools used benefit understanding \\x01 tool \\x01 market works techniques describe certainly \\x01 magic wands \\x01 8 waved data risk management problems provide instant perfect solutions quote riskmetrics technical document \\x01 \\x01 amount sophisticated analytics replace experience professional judgement managing risk \\x01 8 however right tools correctly used make job lot easier',\n",
       "        ...,\n",
       "        ' subject fw london work hi london seems left august sun cold serious looking people expensive etc addition may day riots post office bombing train strike etc mention excitement enron credit would nice know supposed reporting getting loads conflicting messages illustrated forwarded email vasant according slava strategy paper duffie report seems higher priority however vasant seems indicate forwarded email priority moment addition seems lots chaos enron credit houston office even london office brings mind russian proverb learned slava expressed views current state enron credit fish rots head finally would like know exactly want write duffie report want hear enron credit would like hear need us develop private firm model exisiting infrastructure want hear really see hear read etc latter true may need write two reports learning look good would probably make enron credit personnel happy well think said enough look forward feedback thanks iris original message shanbhogue vasant sent friday may 04 2001 3 39 pm mack iris cc dhar amitava subject london work hi iris amitava must told getting swamped work result expect take lead scoping enron credit project making sure infrastructure readied also make sure understand econometric data analysis software side project probably important preparing document duffie right definitely sit ben george actually run software get feel used also need able try potential ways analyzing data amitava help best answer direct questions limited time review documents etc expect amitava get heavily involved data starts coming expect already set infrastructure etc data hope trip going well would extending trip time vasant',\n",
       "        ' subject yen outlook vince followup meeting david port rudi zipter enron investment sk enron maureen wrote attached position paper japanese yen discussed volatility closely tracks fluctuation yen yen position paper intended complement outlook piece broader perspective currencies takes account yen influence asian currencies would like distribute outlook david rudi wanted send initial reaction prior internal distribution thank let know questions comments attached gwyn',\n",
       "        ' subject women cum face click removed'], dtype='<U43816'),\n",
       " array(['spam', 'ham', 'spam', ..., 'spam', 'spam', 'ham'], dtype='<U43816'),\n",
       " array(['spam', 'ham', 'ham', ..., 'ham', 'ham', 'spam'], dtype='<U43816'))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_data(mails)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852d27d2",
   "metadata": {},
   "source": [
    "### Algorithme KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93b0abc",
   "metadata": {},
   "source": [
    "#### Fonction get_count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e09c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(text):\n",
    "    wordCounts = dict()\n",
    "    for word in text.split():\n",
    "        if word in wordCounts:\n",
    "            wordCounts[word] += 1\n",
    "        else:\n",
    "            wordCounts[word] = 1\n",
    "    \n",
    "    return wordCounts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f97e250",
   "metadata": {},
   "source": [
    "#### Fonction euclidean_difference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f4403f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_difference(test_WordCounts, training_WordCounts):\n",
    "    total = 0\n",
    "    for word in test_WordCounts:\n",
    "        if word in test_WordCounts and word in training_WordCounts:\n",
    "            total += (test_WordCounts[word] - training_WordCounts[word])**2\n",
    "            del training_WordCounts[word]\n",
    "        else:\n",
    "            total += test_WordCounts[word]**2\n",
    "    return total**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece68ac6",
   "metadata": {},
   "source": [
    "#### Fonction get_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e61cc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class(selected_Kvalues):\n",
    "    spam_count = 0\n",
    "    ham_count = 0\n",
    "    for value in selected_Kvalues:\n",
    "        if value[0] == \"spam\":\n",
    "            spam_count += 1\n",
    "        else:\n",
    "            ham_count += 1\n",
    "    if spam_count > ham_count:\n",
    "        return \"spam\"\n",
    "    else:\n",
    "        return \"ham\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9726f340",
   "metadata": {},
   "source": [
    "#### Fonction knn_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b3ebc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_classifier(training_data, training_labels, test_data, K,tsize):\n",
    "    print(\"Running KNN Classifier...\")\n",
    "    result = []\n",
    "    counter = 1\n",
    "    # word counts for training email\n",
    "    training_WordCounts = []\n",
    "    for training_text in training_data:\n",
    "        training_WordCounts.append(get_count(training_text))\n",
    "    for test_text in test_data:\n",
    "        similarity = [] # List of euclidean distances\n",
    "        test_WordCounts = get_count(test_text) # word counts fortest email\n",
    "        # Getting euclidean difference\n",
    "        for index in range(len(training_data)):\n",
    "            euclidean_diff =\\\n",
    "            euclidean_difference(test_WordCounts,training_WordCounts[index])\n",
    "            similarity.append([training_labels[index],euclidean_diff])\n",
    "        # Sort list in ascending order based on euclidean difference\n",
    "        similarity = sorted(similarity, key = lambda i:i[1])\n",
    "        # Select K nearest neighbours\n",
    "        selected_Kvalues = []\n",
    "        for i in range(K):\n",
    "            selected_Kvalues.append(similarity[i])\n",
    "        # Predicting the class of email\n",
    "        result.append(get_class(selected_Kvalues))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91660fc3",
   "metadata": {},
   "source": [
    "### Fonction main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64f48047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(K):\n",
    "    data = load_data()\n",
    "    data = preprocess_data(data)\n",
    "    training_data, test_data, training_labels, test_labels = split_data(data)\n",
    "    tsize = len(data)\n",
    "    result = knn_classifier(training_data, training_labels, test_data[:tsize], K, tsize)\n",
    "    accuracy = accuracy_score(test_labels[:tsize], result)\n",
    "    \n",
    "    print(\"training data size\\t: \" + str(len(training_data)))\n",
    "    print(\"test data size\\t\\t: \" + str(len(test_data)))\n",
    "    print(\"K value\\t\\t\\t\\t: \" + str(K))\n",
    "    print(\"Samples tested\\t\\t: \" + str(tsize))\n",
    "    print(\"% accuracy\\t\\t\\t: \" + str(accuracy * 100))\n",
    "    print(\"Number correct\\t\\t: \" + str(int(accuracy * tsize)))\n",
    "    print(\"Number wrong\\t\\t: \" + str(int((1 - accuracy) * tsize)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc92b50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "flag 1: loaded data\n",
      "Preprocessing data...\n",
      "flag 2: preprocessed data\n",
      "Splitting data...\n",
      "['ham' 'ham' 'ham' ... 'spam' 'spam' 'spam']\n",
      "flag 3: splitted data\n",
      "Running KNN Classifier...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-0d99119751cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-27-9420ccc92d09>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(K)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mknn_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtsize\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtsize\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-aa4810b4c934>\u001b[0m in \u001b[0;36mknn_classifier\u001b[1;34m(training_data, training_labels, test_data, K, tsize)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0meuclidean_diff\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0meuclidean_difference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_WordCounts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtraining_WordCounts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[0msimilarity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtraining_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0meuclidean_diff\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# Sort list in ascending order based on euclidean difference\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-f377d4601fa5>\u001b[0m in \u001b[0;36meuclidean_difference\u001b[1;34m(test_WordCounts, training_WordCounts)\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mtraining_WordCounts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             \u001b[0mtotal\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtest_WordCounts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65e1648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
